{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5afc9885",
   "metadata": {
    "id": "5afc9885",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Creating a dataset\n",
    "\n",
    "This tutorial provides a short example on how to create a SeisBench dataset. Datasets can be created from any event catalog and waveform collection. For this example, we download an event catalog for Switzerland through FDSN. We will then download the associated waveforms through FDSN as well. We use built-in SeisBench functions for writing out the dataset in SeisBench format. In this example notebook we aim for an easy example outlining the principles of dataset creation. There are a few further considerations, in particular, for converting larger datasets, that we outline at the end.\n",
    "\n",
    "**Note:** Some familiarity with obspy and its FDSN client is helpful for this tutorial, but not required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d5a03f",
   "metadata": {
    "id": "a5d5a03f",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import seisbench.data as sbd\n",
    "import seisbench.util as sbu\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from obspy import read_events\n",
    "from obspy import read\n",
    "from obspy import Stream\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d14a32",
   "metadata": {
    "id": "91d14a32",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### The event catalog\n",
    "\n",
    "As a first step, we need an event catalog. Here, we are going to use the catalog provided by ETHZ over FDSN. For demonstration purposes, we only use a short time window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08ca2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_base = '/home/ekarkooti/Documents'\n",
    "path_catalog = f'{path_base}/Kaki-test/*.CAT'\n",
    "path_stream = f\"{path_base}/Kaki/raw-gcf-data/\"\n",
    "path_output = f'{path_base}/Kaki-test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d087f36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = read_events(path_catalog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2b537d",
   "metadata": {
    "id": "da2b537d",
    "outputId": "04d9a034-a88f-45fe-b97d-7b20448e3148",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# print(catalog.__str__(print_all=True))\n",
    "print(catalog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe49f98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = [ev for ev in catalog if ev.picks != []]\n",
    "print(len(catalog))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29aaef76",
   "metadata": {
    "id": "29aaef76",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Extracting the event parameters\n",
    "\n",
    "From the catalog, we extract the event parameters and store them into a dictionary. Here, we only extract a few basic parameters on the source and its magnitude - if available. In addition, we define the split of the dataset into training/development/test partitions. We visualize one example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18becd85",
   "metadata": {
    "id": "18becd85",
    "outputId": "80b15fee-195d-468a-fa9f-e80e368c21fa",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_event_params(event):\n",
    "    origin = event.preferred_origin()\n",
    "    mag = event.preferred_magnitude()\n",
    "\n",
    "    source_id = str(event.resource_id)\n",
    "\n",
    "    event_params = {\n",
    "        \"source_id\": source_id,\n",
    "        \"source_origin_time\": str(origin.time),\n",
    "        \"source_origin_uncertainty_sec\": origin.time_errors[\"uncertainty\"],\n",
    "        \"source_latitude_deg\": origin.latitude,\n",
    "        \"source_latitude_uncertainty_km\": origin.latitude_errors[\"uncertainty\"],\n",
    "        \"source_longitude_deg\": origin.longitude,\n",
    "        \"source_longitude_uncertainty_km\": origin.longitude_errors[\"uncertainty\"],\n",
    "        \"source_depth_km\": origin.depth / 1e3            if origin.depth else None,\n",
    "        \"source_depth_uncertainty_km\": origin.depth_errors[\"uncertainty\"] / 1e3           if origin.depth else None,\n",
    "    }\n",
    "\n",
    "    if mag is not None:\n",
    "        event_params[\"source_magnitude\"] = mag.mag\n",
    "        event_params[\"source_magnitude_uncertainty\"] = mag.mag_errors[\"uncertainty\"]\n",
    "        event_params[\"source_magnitude_type\"] = mag.magnitude_type\n",
    "        event_params[\"source_magnitude_author\"] = mag.creation_info.agency_id\n",
    "\n",
    "        if str(origin.time) < \"2015-01-07\":\n",
    "            split = \"train\"\n",
    "        elif str(origin.time) < \"2015-01-08\":\n",
    "            split = \"dev\"\n",
    "        else:\n",
    "            split = \"test\"\n",
    "        event_params[\"split\"] = split\n",
    "    return event_params\n",
    "\n",
    "# print(catalog[0].preferred_magnitude() is not None)\n",
    "# get_event_params(catalog[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917b7f36",
   "metadata": {
    "id": "917b7f36",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Extracting the trace parameters\n",
    "\n",
    "From each pick, we extract parameters about the trace and store them in a dictionary. Again, we only extract very basic parameters. We visualize one example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0e47c0",
   "metadata": {
    "id": "ef0e47c0",
    "outputId": "cf98a9a5-eed4-4fca-ce2f-ff7cb4f9509d",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_trace_params(pick):\n",
    "    net = pick.waveform_id.network_code\n",
    "    sta = pick.waveform_id.station_code\n",
    "\n",
    "    trace_params = {\n",
    "        \"station_network_code\": net,\n",
    "        \"station_code\": sta,\n",
    "        \"trace_channel\": pick.waveform_id.channel_code[:2],\n",
    "        \"station_location_code\": pick.waveform_id.location_code,\n",
    "    }\n",
    "\n",
    "    return trace_params\n",
    "\n",
    "# get_trace_params(catalog[0].picks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b564335e",
   "metadata": {
    "id": "b564335e",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Downloading the waveforms\n",
    "\n",
    "As a last step, we need to access the waveforms. As for the catalog, we download the waveforms from ETHZ via FDSN. Note that not for all picks we can expect to have waveforms available through FDSN, so we just return empty streams if no data is available. We visualize one example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d2a356",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_stream_mapper(obspy_stream, networks, stations, locations, channels):\n",
    "    for tr in obspy_stream:\n",
    "        tr.stats.network = networks[tr.stats.network]\n",
    "        tr.stats.station = stations[tr.stats.station]\n",
    "        tr.stats.location = locations[tr.stats.location]\n",
    "        tr.stats.channel = channels[tr.stats.channel]\n",
    "\n",
    "def reversing_dictionary(dictionary):\n",
    "    return {v:k for k, v in dictionary.items()}\n",
    "\n",
    "networks = {'': ''}\n",
    "stations = {'6210': 'HANA',# 6210-Hana-shour  \n",
    "            '6249': 'SORM',# 6249-Sarmak ??? SORM\n",
    "            '6260': 'CHAH',# 6260-Chahgah     \n",
    "            '6270': 'LAVA',# 6270-Lavar\n",
    "            '6215': 'JASH',# 6215-Jashk       \n",
    "            '6251': 'SANA',# 6251-Sana              \n",
    "            '6261': 'SHON',# 6261-Shonbeh     \n",
    "            '6289': 'ABDA',# 6289-Abdan\n",
    "            '6218': 'KERD',# 6218-Kerdelan    \n",
    "            '6252': 'BONY',# 6252-Bonyad            \n",
    "            '6266': 'KARD',# 6266-Kardaneh\n",
    "            '6219': 'DOHO',# 6219-Dohouk      \n",
    "            '6255': 'BOBM',# 6255-Babmonir          \n",
    "            '6268': 'ESLA',# 6268-Eslam-Abad\n",
    "            '6226': 'BAGH',# 6226-Baghan      \n",
    "            '6259': 'GENK',# 6259-Genkhak-Sheikhha  \n",
    "            '6270': 'DARV',# 6270-drveshei\n",
    "            }\n",
    "\n",
    "\n",
    "locations = {'': ''}\n",
    "# channels = {'HHZ': 'SZ',\n",
    "#             'HHN': 'SN',\n",
    "#             'HHE': 'SE'}\n",
    "channels = {'HHZ': 'HHZ',\n",
    "            'HHN': 'HHN',\n",
    "            'HHE': 'HHE'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c95461d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b9600e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stationsr = reversing_dictionary(stations)\n",
    "# station_code = stationsr.get('SORM', None)\n",
    "# station_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cf41f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_waveforms(pick, root):\n",
    "    stationsr = reversing_dictionary(stations)\n",
    "    station_code = stationsr.get(pick.waveform_id.station_code)\n",
    "    if station_code==None:\n",
    "        print(f'Station Not Found.\\n{pick.waveform_id.station_code}\\n')\n",
    "        with open('reading-data-problem.txt', 'a') as f:\n",
    "            f.write(f'Station Not Found.\\n{pick.waveform_id.station_code}\\n')\n",
    "        return Stream()\n",
    "    time = pick.time.strftime('%Y%m%d_%H')\n",
    "    st = Stream()\n",
    "    for channel in ['e', 'n', 'z']:\n",
    "        path_data = f'{root}/{station_code}*/gcf/{time}00{channel}.gcf'\n",
    "        try:\n",
    "            st += read(path_data)\n",
    "        except Exception as error:\n",
    "            print(f'Skip Data.\\n{error}\\n{path_data}\\n')\n",
    "            with open('reading-data-problem.txt', 'a') as f:\n",
    "                f.write(f'Skip Data.\\n{error}\\n{path_data}\\n')\n",
    "    # st_trim = st.slice(starttime=pick.time-time_before,\n",
    "    #                    endtime=pick.time+time_after)\n",
    "    \n",
    "    return st\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be38a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_data(st, pick, before, after):\n",
    "    st.trim(starttime=pick.time-before,\n",
    "            endtime=pick.time+after,\n",
    "            pad=True,\n",
    "            nearest_sample=True,\n",
    "            fill_value=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c45894",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_data(st):\n",
    "    st.merge(-1)\n",
    "    st.detrend('constant')\n",
    "    st.merge(fill_value=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be45494c",
   "metadata": {
    "id": "be45494c",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Writing to SeisBench format\n",
    "\n",
    "Now, we can combine all the above functions together to write a dataset in SeisBench format. For this, we first need to define the path. For this example, we are using the current working directory. A dataset consists of 2 components:\n",
    " - a metadata file, always called `metadata.csv`, which contains all the associated properties of the waveform examples (e.g. trace parameters, source parameters etc.).\n",
    " - a waveforms file, always called `waveforms.hdf5`, containing the raw waveforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d0d5f8",
   "metadata": {
    "id": "30d0d5f8",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "base_path = Path(path_output)\n",
    "metadata_path = base_path / \"metadata.csv\"\n",
    "waveforms_path = base_path / \"waveforms.hdf5\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea69c41a",
   "metadata": {
    "id": "ea69c41a",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "To write the dataset, we use the `WaveformDataWriter` provided by SeisBench. The writer should always be used as a context manager, i.e., using the `with` statement, as shown below. This is to ensure files are properly clsoed after writing and teardown and cleanup operations are always called when exiting the context manager.\n",
    "\n",
    "First, we need to set the data format for our dataset. We do this by assigning a dictionary to the `writer.data_format` group.\n",
    "\n",
    "Next, we iterate over all event and all picks in the events. Using the functions above, we generate the event and trace metadata and download the waveforms. We then convert the waveforms to a numpy array using the function `stream_to_array` provided in `seisbench.util`.\n",
    "\n",
    "As a last step, we hand the event metadata and the waveforms as numpy array over to the writer using `add_trace`. The writer then automatically takes care of writing out the data in the correct format. It also takes care of performance optimisations that we outline in the further considerations below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba60659",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_picks(picks, station_name):\n",
    "    picks = [pick for pick in picks\n",
    "             if pick.waveform_id.station_code==station_name]\n",
    "    picks = sorted(picks,\n",
    "                   key= lambda p: p.time)\n",
    "    return picks\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533508ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_picks_time_difference(picks):\n",
    "    picks_time = [pick.time for pick in picks]\n",
    "    picks_time = sorted(picks_time)\n",
    "    picks_difftime = [time-picks_time[0] for time in picks_time]\n",
    "    return picks_difftime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041c98fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def checking_equal_sps(stream):\n",
    "    sps = stream[0].stats.sampling_rate\n",
    "    assert all(tr.stats.sampling_rate == sps for tr in stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701fc683",
   "metadata": {},
   "outputs": [],
   "source": [
    "before = 5\n",
    "duration = 30\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbec34f6",
   "metadata": {
    "id": "dbec34f6",
    "outputId": "d50fa6fd-ed8a-4932-c9db-c1fad4246dd1",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Iterate over events and picks, write to SeisBench format\n",
    "with sbd.WaveformDataWriter(metadata_path, waveforms_path) as writer:\n",
    "\n",
    "    # Define data format\n",
    "    writer.data_format = {\n",
    "        \"dimension_order\": \"CW\",\n",
    "        \"component_order\": \"ZNE\",\n",
    "        \"measurement\": \"velocity\",\n",
    "        \"unit\": \"counts\",\n",
    "        \"instrument_response\": \"not restituted\",\n",
    "    }\n",
    "    n_all = len(catalog)\n",
    "    for index, event in enumerate(catalog):\n",
    "        # if index < 1050:\n",
    "        #     continue\n",
    "        if index % 50 == 0:\n",
    "            print(f'{index} of {n_all} {index/n_all:.2f}%')\n",
    "            # if index == 100:\n",
    "            #     break\n",
    "\n",
    "        event_params = get_event_params(event)\n",
    "        stations_in_event = {pick.waveform_id.station_code for pick in event.picks}\n",
    "        for station_name in stations_in_event:\n",
    "            picks = select_picks(picks=event.picks,\n",
    "                                 station_name=station_name)\n",
    "            ###\n",
    "            time_diff = get_picks_time_difference(picks)\n",
    "            if max(time_diff) >= 60:\n",
    "                print(f'losing pick, maximume is: {max(time_diff)}')\n",
    "            ###\n",
    "            pick = picks[0]\n",
    "            trace_params = get_trace_params(pick)\n",
    "            waveforms = get_waveforms(pick, path_stream)\n",
    "            ### Preprocessing waveform\n",
    "            rename_stream_mapper(waveforms, networks, stations, locations, channels)\n",
    "            preprocessing_data(st=waveforms)\n",
    "            random = np.random.uniform(-before/2, before/2)\n",
    "            trim_data(waveforms, pick, before=before-random, after=duration-before+random)\n",
    "            ### Check remaining data\n",
    "            if len(waveforms) == 0:\n",
    "                # No waveform data available\n",
    "                continue\n",
    "            ###\n",
    "            sampling_rate = waveforms[0].stats.sampling_rate\n",
    "            # Check that the traces have the same sampling rate\n",
    "            checking_equal_sps(stream=waveforms)\n",
    "\n",
    "            actual_t_start, data, _ = sbu.stream_to_array(\n",
    "                waveforms,\n",
    "                component_order=writer.data_format[\"component_order\"],\n",
    "            )\n",
    "            trace_params[\"trace_sampling_rate_hz\"] = sampling_rate\n",
    "            trace_params[\"trace_start_time\"] = str(actual_t_start)\n",
    "\n",
    "            for pick in picks:\n",
    "                sample = (pick.time - actual_t_start) * sampling_rate\n",
    "                trace_params[f\"trace_{pick.phase_hint}_arrival_sample\"] = int(sample)\n",
    "                trace_params[f\"trace_{pick.phase_hint}_status\"] = pick.evaluation_mode\n",
    "\n",
    "            writer.add_trace({**event_params, **trace_params}, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ba93ee",
   "metadata": {
    "id": "11ba93ee",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Loading the dataset\n",
    "\n",
    "Now that the dataset conversion is finished, we can check it by simply loading it. Here we load the dataset, print the metadata and visualize the first waveform together with the annotated pick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517e21b9",
   "metadata": {
    "id": "517e21b9",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = sbd.WaveformDataset(base_path, sampling_rate=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd22a167",
   "metadata": {
    "id": "bd22a167",
    "outputId": "73f9f276-334b-4c71-a8d1-7b55ad7bc9ab",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Training examples:\", len(data.train()))\n",
    "print(\"Development examples:\", len(data.dev()))\n",
    "print(\"Test examples:\", len(data.test()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4d4bce",
   "metadata": {
    "id": "3e4d4bce",
    "outputId": "56570c19-a678-4e6a-89b1-df8087434807",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "targets = [key for key in data.metadata.keys() if 'arrival' in key]\n",
    "data.metadata[targets]\n",
    "# data.metadata.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3851d1a5",
   "metadata": {
    "id": "3851d1a5",
    "outputId": "876eb18a-bd36-4be6-fd7d-13296fea9cd3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def cmap(phase_hint):\n",
    "    c = {'Pg': 'r', 'Sg': 'b', 'AML': 'c'}\n",
    "    return c.get(phase_hint, 'y')\n",
    "\n",
    "for ii, metadata in data.metadata.iterrows():\n",
    "    # print(metadata)\n",
    "    fig = plt.figure(figsize=(10, 7))\n",
    "    ax = fig.add_subplot(111)\n",
    "    trace = data.get_waveforms(ii)\n",
    "    print(trace.shape)\n",
    "    ax.plot(trace.T)\n",
    "    targets = [key for key in metadata.keys() if 'arrival' in key]\n",
    "    targets = [key for key in targets if not np.isnan(metadata[key])]\n",
    "    # print(targets, metadata[targets])\n",
    "    for target in targets:\n",
    "        phase_hint = target.split('_')[1]\n",
    "        ax.axvline(metadata[target], lw=3, c=cmap(phase_hint), label=phase_hint)\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2930fb",
   "metadata": {
    "id": "1c2930fb",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Considerations for converting datasets\n",
    "\n",
    "As outlined above, this tutorial provides a very minimal example on converting a dataset. Here we outline additional consideration that should be taken into account when preparing a dataset.\n",
    "\n",
    "- **Grouping picks**: In this example, we created one trace for each pick. Naturally, traces will overlap if multiple picks, e.g., P and S phases, are available for an event at a station. For an example implementation of this grouping operation, have a look [here](https://github.com/seisbench/seisbench/blob/df94dcd86ce66d6a2ee2bd00da3857259fe579bd/seisbench/data/ethz.py#L109) and in the subsequent lines.\n",
    "- **Adding station information**: In this example, we added no station information except its name. In practice, it will often be helpful for users to incorporate, for example, the location of the station. We skipped this step here, because it requires loading station inventories through FDSN. For an example implementation, have a look [here](https://github.com/seisbench/seisbench/blob/df94dcd86ce66d6a2ee2bd00da3857259fe579bd/seisbench/data/ethz.py#L315).\n",
    "- **Memory requirements**: Internally, the `WaveformDataWriter` writes out the the waveforms continuously in blocks (see point below), but keeps all metadata in memory until the dataset is complete. For very large datasets (or very detailed metadata) this can result in several gigabytes of memory consumption. If you are writing such datasets, make sure the available memory on your machine is sufficient.\n",
    "- **Waveform blocks**: Instead of writing each waveform separately, waveforms are written out in blocks. This massively improves IO performance. Have a look at [the documentation](https://seisbench.readthedocs.io/en/stable/pages/data_format.html#traces-blocks) for details on the strategy. We expect that in nearly all cases using the default setting will be a good choice.\n",
    "- **FDSN considerations**: When converting very large datasets, the performance might be limited by the performance of the FDSN webservice. Unfortunately, downloading lots of short waveforms (as required for many machine learning applications) does not seem to be the most favorable use case for FDSN. This leads to rather slow performance when naively downloading the waveforms as outlined above. Instead, it is often helpful to issue [bulk requests](https://docs.obspy.org/master/packages/autogen/obspy.clients.fdsn.client.Client.get_waveforms_bulk.html). In addition, it might be a good choice to first download the waveforms and cache them locally, for example, in .mseed format, and then convert them to SeisBench.\n",
    "\n",
    "For further details on the data format, check out [the data format specification in the SeisBench documentation](https://seisbench.readthedocs.io/en/stable/pages/data_format.html#traces-blocks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbde789",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cmap(phase_hint):\n",
    "    c = {'Pg': 'r', 'Sg': 'b', 'AML': 'c'}\n",
    "    return c.get(phase_hint, 'y')\n",
    "\n",
    "ii = 7\n",
    "metadata = data.metadata.iloc[ii]\n",
    "# print(metadata)\n",
    "fig, axes = plt.subplots(3, 1, figsize=(10, 5), sharex=True)\n",
    "trace = data.get_waveforms(ii)\n",
    "print(trace.shape)\n",
    "for jj in range(3):\n",
    "    axes[jj].plot(trace.T[:, jj], c='k', lw=0.5)\n",
    "    axes[jj].patch.set_visible(False)\n",
    "    axes[jj].axis('off')\n",
    "targets = [key for key in metadata.keys() if 'arrival' in key]\n",
    "targets = [key for key in targets if not np.isnan(metadata[key])]\n",
    "# print(targets, metadata[targets])\n",
    "for target in targets:\n",
    "    phase_hint = target.split('_')[1]\n",
    "    for jj in range(3):\n",
    "        axes[jj].axvline(metadata[target], lw=2, c=cmap(phase_hint), label=phase_hint[0])\n",
    "axes[1].legend()\n",
    "plt.xlim([400, 2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390b57d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
